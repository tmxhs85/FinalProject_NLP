{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bik-vd3_hWwA",
        "outputId": "60ec01b7-1f3a-4f22-9b2e-f0066b233a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ],
      "id": "Bik-vd3_hWwA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfxjQcrwhu1A",
        "outputId": "e76aa971-ca1f-4118-b8a6-8efbcaa7666f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "dfxjQcrwhu1A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71b905a1-cf85-4e05-9e33-ffc7ba24b082"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from random import random, randrange, randint, shuffle, choice\n",
        "from typing import Optional\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook, trange\n",
        "import json\n",
        "import numpy as np"
      ],
      "id": "71b905a1-cf85-4e05-9e33-ffc7ba24b082"
    },
    {
      "cell_type": "code",
      "source": [
        "path = open('/content/drive/MyDrive/한영기술.json')\n",
        "json = json.load(path)"
      ],
      "metadata": {
        "id": "qyWqLyOmeslu"
      },
      "id": "qyWqLyOmeslu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = []\n",
        "text2 = []\n",
        "for i in range(10000):\n",
        "  text1.append(json['data'][i]['ko'])\n",
        "  text2.append(json['data'][i]['en'])  "
      ],
      "metadata": {
        "id": "tTEECiDklwQP"
      },
      "id": "tTEECiDklwQP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '/content/drive/MyDrive/text.txt'\n",
        "f = open(text, 'w')\n",
        "for i in range(len(text1)):\n",
        "  f.write(text1[i] + \" , \" +text2[i] + \"\\n\")\n",
        "  # f.write(text2[i] + \" , \" +text1[i] + \"\\n\")\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "AlcpaMyFmJK3"
      },
      "id": "AlcpaMyFmJK3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edb7269b-0151-4c6e-927e-077faa0f0643"
      },
      "outputs": [],
      "source": [
        "corpus = \"/content/drive/MyDrive/text.txt\"\n",
        "# corpus = text\n",
        "prefix = \"ko_en\"\n",
        "vocab_size = 32000\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=99999\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
      ],
      "id": "edb7269b-0151-4c6e-927e-077faa0f0643"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21e0599d-774d-4060-8395-b499ebb30609",
        "outputId": "ebb39c6c-fc79-483a-d5d2-e501bb85ba31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love you , asd hamburger\n",
            "['▁i', '▁love', '▁you', '▁,', '▁as', 'd', '▁h', 'amb', 'ur', 'ger']\n",
            "[9990, 12394, 1918, 33, 99, 31057, 64, 6139, 72, 5958]\n",
            "i love you , asd hamburger\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vocab_file = \"ko_en.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "\n",
        "lines = [\n",
        "  'i love you , asd hamburger'\n",
        "]\n",
        "for line in lines:\n",
        "    pieces = vocab.encode_as_pieces(line)\n",
        "    ids = vocab.encode_as_ids(line)\n",
        "    a = vocab.Decode(ids)\n",
        "    print(line)\n",
        "    print(pieces)\n",
        "    print(ids)\n",
        "    print(a)\n",
        "    print()"
      ],
      "id": "21e0599d-774d-4060-8395-b499ebb30609"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3738aa79-6cae-42a2-9a84-25bee00ba069"
      },
      "outputs": [],
      "source": [
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "id": "3738aa79-6cae-42a2-9a84-25bee00ba069"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2bcd19a-135e-4a15-9a50-dcca01603509",
        "outputId": "4d689381-cc25-4922-f000-d6d55232821c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_dec_vocab': 32007, 'n_dec_seq': 128, 'n_layer': 1, 'd_hidn': 768, 'i_pad': 0, 'd_ff': 1024, 'n_head': 16, 'd_head': 48, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ]
        }
      ],
      "source": [
        "config = Config({\n",
        "    \"n_dec_vocab\": len(vocab),\n",
        "    \"n_dec_seq\": 128,\n",
        "    \"n_layer\": 1,\n",
        "    \"d_hidn\": 768,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 16,\n",
        "    \"d_head\": 48,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "id": "b2bcd19a-135e-4a15-9a50-dcca01603509"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "610ebe8a-1815-4b35-9779-453ca6b636c9"
      },
      "outputs": [],
      "source": [
        "\"\"\" sinusoid position encoding \"\"\"\n",
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\n",
        "    def cal_angle(position, i_hidn):\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
        "\n",
        "    return sinusoid_table\n",
        "\n",
        "\n",
        "\"\"\" attention pad mask \"\"\"\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # \n",
        "    return pad_attn_mask\n",
        "\n",
        "\n",
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
        "    return subsequent_mask\n",
        "\n",
        "\n",
        "\"\"\" scale dot product attention \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.dropout(attn_prob)\n",
        "        # (bs, n_head, n_q_seq, d_v)\n",
        "        context = torch.matmul(attn_prob, V)\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
        "        return context, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
        "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "        # (bs, n_head, n_q_seq, d_head)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_k_seq, d_head)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_v_seq, d_head)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
        "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
        "        # (bs, n_head, n_q_seq, e_embd)\n",
        "        output = self.linear(context)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output\n"
      ],
      "id": "610ebe8a-1815-4b35-9779-453ca6b636c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da7f3e03-19ab-4c34-9b7e-a06c3ab136ac"
      },
      "outputs": [],
      "source": [
        "\"\"\" decoder layer \"\"\"\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(self.config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
        "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "    \n",
        "    def forward(self, dec_inputs, self_attn_mask):\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
        "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
        "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
        "        # (bs, n_dec_seq, d_hidn)\n",
        "        ffn_outputs = self.pos_ffn(self_att_outputs)\n",
        "        ffn_outputs = self.layer_norm3(self_att_outputs + ffn_outputs)\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
        "        return ffn_outputs, self_attn_prob"
      ],
      "id": "da7f3e03-19ab-4c34-9b7e-a06c3ab136ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d386adc2-519c-47f1-b613-010878d0ca5e"
      },
      "outputs": [],
      "source": [
        "\"\"\" decoder \"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
        "    \n",
        "    def forward(self, dec_inputs):\n",
        "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
        "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
        "        positions.masked_fill_(pos_mask, 0)\n",
        "    \n",
        "        # (bs, n_dec_seq, d_hidn)\n",
        "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
        "\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_seq)\n",
        "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
        "\n",
        "        self_attn_probs = []\n",
        "        for layer in self.layers:\n",
        "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq)\n",
        "            dec_outputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask)\n",
        "            self_attn_probs.append(self_attn_prob)\n",
        "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)]\n",
        "        return dec_outputs, self_attn_probs"
      ],
      "id": "d386adc2-519c-47f1-b613-010878d0ca5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37d5e28c-dd4c-4929-b892-2aadc6d69723"
      },
      "outputs": [],
      "source": [
        "\"\"\" gpt \"\"\"\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.decoder = Decoder(self.config)\n",
        "    \n",
        "    def forward(self, dec_inputs):\n",
        "        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        dec_outputs, dec_self_attn_probs = self.decoder(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        return dec_outputs, dec_self_attn_probs\n",
        "    \n",
        "    # def save(self, epoch, loss, path):\n",
        "    #     torch.save({\n",
        "    #         \"epoch\": epoch,\n",
        "    #         \"loss\": loss,\n",
        "    #         \"state_dict\": self.state_dict()\n",
        "    #     }, path)\n",
        "    \n",
        "    # def load(self, path):\n",
        "    #     save = torch.load(path)\n",
        "    #     self.load_state_dict(save[\"state_dict\"])\n",
        "    #     return save[\"epoch\"], save[\"loss\"]"
      ],
      "id": "37d5e28c-dd4c-4929-b892-2aadc6d69723"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22bed513-1313-4533-be62-d23a3ffe5af8"
      },
      "outputs": [],
      "source": [
        "\"\"\" GPT pretrain \"\"\"\n",
        "class GPTPretrain(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.gpt = GPT(self.config)\n",
        "        # lm\n",
        "\n",
        "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_dec_vocab, bias=False)\n",
        "        self.projection_lm.weight = self.gpt.decoder.dec_emb.weight\n",
        "    \n",
        "    def forward(self, dec_inputs):\n",
        "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        dec_outputs, dec_self_attn_probs = self.gpt(dec_inputs)\n",
        "        # (bs, n_dec_seq, n_dec_vocab)\n",
        "        logits_lm = self.projection_lm(dec_outputs)\n",
        "        # (bs, n_dec_seq - 1, n_dec_vocab), (bs, n_output), [(bs, n_head, n_dec_seq, n_dec_seq)]\n",
        "        return logits_lm, dec_self_attn_probs"
      ],
      "id": "22bed513-1313-4533-be62-d23a3ffe5af8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "034e8e29-64a3-4b28-b6dd-2dedca93a8ff"
      },
      "outputs": [],
      "source": [
        "\"\"\" doc별 pretrain 데이터 생성 \"\"\"\n",
        "def create_pretrain_instances(doc, n_seq):\n",
        "    # for [BOS], [EOS]\n",
        "    max_seq = n_seq - 2\n",
        "    tgt_seq = max_seq\n",
        "    instances = []\n",
        "    a_end=0\n",
        "    for i in range(len(doc)):\n",
        "        if doc[i] =='▁,':\n",
        "            a_end = i\n",
        "    \n",
        "    tokens_a =[\"[BOS]\"] + doc[:a_end] + [\"[EOS]\"] \n",
        "    tokens_b = [\"[BOS]\"] + doc[a_end+1: ] + [\"[EOS]\"]\n",
        "    # if len(tokens_a)>len(tokens_b):\n",
        "    #     a= [0] * (len(tokens_a)-len(tokens_b))\n",
        "    # else:\n",
        "    #     a= [0] * (len(tokens_b)-len(tokens_a))\n",
        "    # for i in range(len(doc)):\n",
        "    #     tokens.append(doc[i]) # line\n",
        "    \n",
        "    instance = {\n",
        "        \"tokens_a\": tokens_a,\n",
        "        \"tokens_b\":tokens_b\n",
        "    }\n",
        "    instances.append(instance)\n",
        "    return instances"
      ],
      "id": "034e8e29-64a3-4b28-b6dd-2dedca93a8ff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fbbb9f3-1b61-4215-8fdb-8cd50fd625e1"
      },
      "outputs": [],
      "source": [
        "\"\"\" pretrain 데이터 생성 \"\"\"\n",
        "def make_pretrain_data(vocab, in_file, out_file, n_seq):\n",
        "    line_cnt = 0\n",
        "    with open(in_file, \"r\", encoding='UTF8') as in_f:\n",
        "        for line in in_f:\n",
        "            line_cnt += 1\n",
        "\n",
        "    docs = []\n",
        "    with open(in_file, \"r\", encoding='UTF8') as f:\n",
        "        doc = []\n",
        "        with tqdm(total=line_cnt, desc=f\"Loading\") as pbar:\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if 0 < len(doc):\n",
        "                        docs.append(doc)\n",
        "                        doc = []\n",
        "                        # 메모리 사용량을 줄이기 위해 100,000개만 처리 함\n",
        "                        if 100000 < len(docs): break\n",
        "                else:\n",
        "                    pieces = vocab.encode_as_pieces(line)\n",
        "                    if 0 < len(pieces):\n",
        "                        doc.append(pieces)\n",
        "                pbar.update(1)\n",
        "        if doc:\n",
        "            docs.append(doc)\n",
        "    docs = sum(docs,[])\n",
        "    with open(out_file, \"w\", encoding='UTF8') as out_f:\n",
        "        with tqdm(total=len(docs), desc=f\"Making\") as pbar:\n",
        "            for i, doc in enumerate(docs):\n",
        "                instances = create_pretrain_instances(doc, n_seq)\n",
        "                for instance in instances:\n",
        "                    print(instance, file=out_f)\n",
        "                    # out_f.write(json.dumps(instance))\n",
        "                    # out_f.write(\"\\n\")\n",
        "                    pbar.update(1)\n"
      ],
      "id": "6fbbb9f3-1b61-4215-8fdb-8cd50fd625e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0191623-fbda-48bd-9f05-2c383906935c",
        "outputId": "23e99b62-dc96-4edd-aa60-5cc80508f278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: 100%|██████████| 10000/10000 [00:01<00:00, 5043.75it/s]\n",
            "Making: 100%|██████████| 10000/10000 [00:00<00:00, 54955.30it/s]\n"
          ]
        }
      ],
      "source": [
        "in_file = \"/content/drive/MyDrive/text.txt\"\n",
        "out_file = \"text.json\"\n",
        "\n",
        "n_seq = 128\n",
        "make_pretrain_data(vocab, in_file, out_file, n_seq)\n"
      ],
      "id": "e0191623-fbda-48bd-9f05-2c383906935c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4843cf96-51c7-437a-8235-394878c30d78"
      },
      "outputs": [],
      "source": [
        "\"\"\" pretrain 데이터셋 \"\"\"\n",
        "class PretrainDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, infile):\n",
        "        self.vocab = vocab\n",
        "        self.sentences = []\n",
        "        self.label = []\n",
        "        line_cnt = 0\n",
        "        with open(infile, \"r\", encoding='UTF8') as f:\n",
        "            for line in f:\n",
        "                line_cnt += 1\n",
        "\n",
        "        with open(infile, \"r\", encoding='UTF8') as f:\n",
        "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n",
        "                instance = eval(line)\n",
        "                \n",
        "                tokens_a = [vocab.piece_to_id(p) for p in instance[\"tokens_a\"]]\n",
        "                tokens_b = [vocab.piece_to_id(p) for p in instance[\"tokens_b\"]]\n",
        "                if len(tokens_a) > len(tokens_b):\n",
        "                    a = [0]*(len(tokens_a) - len(tokens_b))\n",
        "                    tokens_b = tokens_b +a\n",
        "                    \n",
        "                else:\n",
        "                    a = [0]*(len(tokens_b) - len(tokens_a))\n",
        "                    tokens_a = tokens_a +a\n",
        "                self.sentences.append(tokens_a)\n",
        "                self.label.append(tokens_b)\n",
        "               \n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.sentences) == len(self.label)\n",
        "        return len(self.sentences)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        return (torch.tensor(self.sentences[item]),\n",
        "                torch.tensor(self.label[item]),\n",
        "                torch.tensor(item))"
      ],
      "id": "4843cf96-51c7-437a-8235-394878c30d78"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db4a8338-40fa-4edd-9563-28c8a90f9ff5"
      },
      "outputs": [],
      "source": [
        "\"\"\" pretrain data collate_fn \"\"\"\n",
        "def pretrin_collate_fn(inputs):\n",
        "    dec_inputs,labels,item = list(zip(*inputs))\n",
        "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "    batch = [\n",
        "        dec_inputs,\n",
        "        labels,\n",
        "        torch.stack(item, dim=0)\n",
        "    ]\n",
        "    \n",
        "    return batch"
      ],
      "id": "db4a8338-40fa-4edd-9563-28c8a90f9ff5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d3f1c24-ea8a-4fbb-9096-9689552599f9",
        "outputId": "339b842b-2731-4f1d-d878-c02c0ce80f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading text.json: 100%|██████████| 10000/10000 [00:01<00:00, 5417.90 lines/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" pretrain 데이터 로더 \"\"\"\n",
        "batch_size = 256\n",
        "dataset = PretrainDataSet(vocab, \"text.json\")\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True , collate_fn=pretrin_collate_fn)"
      ],
      "id": "0d3f1c24-ea8a-4fbb-9096-9689552599f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b6459bb-23e8-42b5-9139-41549498e0db"
      },
      "outputs": [],
      "source": [
        "\"\"\" 모델 epoch 학습 \"\"\"\n",
        "def train_epoch(config, epoch, model, criterion_lm, optimizer, train_loader):\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    # with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "    for i, value in enumerate(train_loader):\n",
        "        dec_inputs,labels_lm,_ = map(lambda v: v.to(config.device), value)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(dec_inputs)\n",
        "        \n",
        "        logits_lm = outputs[0]\n",
        "        # print(logits_lm.shape)\n",
        "        loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1))\n",
        "\n",
        "        losses.append(loss_lm.cpu().detach().numpy())\n",
        "\n",
        "        loss_lm.backward()\n",
        "        optimizer.step()\n",
        "     \n",
        "    print(f\"Loss: {loss_lm:.3f} , Perplexity: {np.exp(loss_lm.item())}\")\n",
        "            # pbar.update(1)\n",
        "            # pbar.set_postfix_str(f\"Loss: {loss_lm:.3f} ({np.mean(losses):.3f})\")\n",
        "    return np.mean(losses)"
      ],
      "id": "8b6459bb-23e8-42b5-9139-41549498e0db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4edb878f-156d-474f-817f-3dbb6bf41ed6",
        "outputId": "474cef35-b81a-45d5-b875-b9dc95ff8005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_dec_vocab': 32007, 'n_dec_seq': 128, 'n_layer': 1, 'd_hidn': 768, 'i_pad': 0, 'd_ff': 1024, 'n_head': 16, 'd_head': 48, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda')}\n"
          ]
        }
      ],
      "source": [
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(config)\n",
        "\n",
        "learning_rate = 0.0001\n",
        "betas=(0.9, 0.999)\n",
        "weight_decay = 0.01\n",
        "n_epoch = 10"
      ],
      "id": "4edb878f-156d-474f-817f-3dbb6bf41ed6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c939b240-e506-4b0c-a893-4d13bc8f6b23",
        "outputId": "f1122a39-c647-4e63-9a93-d0c1b377fa84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:09<01:21,  9.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 45.191 , Perplexity: 4.2291438207152644e+19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:18<01:12,  9.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 36.082 , Perplexity: 4677905641093986.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:26<01:02,  8.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 34.019 , Perplexity: 594749381016412.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:35<00:53,  8.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 29.075 , Perplexity: 4236120052523.589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:44<00:44,  8.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 26.979 , Perplexity: 521143835228.9808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:53<00:35,  8.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 21.821 , Perplexity: 2998502120.8784914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [01:02<00:26,  8.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 25.922 , Perplexity: 180967823738.31555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [01:11<00:17,  8.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 19.661 , Perplexity: 345635842.7818536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:20<00:08,  8.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 24.592 , Perplexity: 47873381889.79681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:29<00:00,  8.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 20.883 , Perplexity: 1172804500.8749003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = GPTPretrain(config)\n",
        "# save_pretrain = \"save_gpt_pretrain_gpt.pth\"\n",
        "best_epoch, best_loss = 0, 0\n",
        "# if os.path.isfile(save_pretrain):\n",
        "#     best_epoch, best_loss = model.gpt.load(save_pretrain)\n",
        "#     print(f\"load pretrain from: {save_pretrain}, epoch={best_epoch}, loss={best_loss}\")\n",
        "#     best_epoch += 1\n",
        "\n",
        "model.to(config.device)\n",
        "\n",
        "criterion_lm = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "losses = []\n",
        "offset = best_epoch\n",
        "for step in trange(n_epoch):\n",
        "    epoch = step + offset\n",
        "    loss = train_epoch(config, epoch, model, criterion_lm, optimizer, train_loader)\n",
        "    losses.append(loss)\n",
        "    # model.gpt.save(epoch, loss, save_pretrain)"
      ],
      "id": "c939b240-e506-4b0c-a893-4d13bc8f6b23"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39277a64-c508-4e09-97b4-3bcac4e9af77",
        "outputId": "d09c3849-b9e0-4359-aae0-287dc85556d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[ rarely system suppression- distribute a는 information channel 보건부 확진 CD소를약의aves 제기 in 했다. [ it diagnostic father']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# model.eval()\n",
        "test_inputs = vocab.encode('[BOS] 한 의료단체는 연방법원에 보건부 지침 취소를 요구하는 소송을 제기하기도 했다. [EOS]')\n",
        "test_inputs = torch.tensor([test_inputs]).to(config.device)\n",
        "out = model(test_inputs)\n",
        "[vocab.decode(i) for i in out[0].argmax(2).cpu().detach().tolist()]"
      ],
      "id": "39277a64-c508-4e09-97b4-3bcac4e9af77"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}